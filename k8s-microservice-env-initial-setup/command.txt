Start Docker
=======================================================================================
sudo -i
systemctl stop docker
nohup sudo dockerd -H unix:///var/run/docker.sock -H tcp://127.0.0.1:2375 &

Start zipkin in bare metal
=======================================================================================
export JAVA_HOME=/home/das/Documents/myLibs/jdk1.8.0_151
export PATH=${JAVA_HOME}/bin:${PATH}
java -jar /home/das/Documents/myLibs/zipkin/zipkin-server-2.11.5-exec.jar

Start prometheus time-series database in bare metal
=======================================================================================
export JAVA_HOME=/home/das/Documents/myLibs/jdk1.8.0_151
export PATH=${JAVA_HOME}/bin:${PATH}
/home/das/Documents/myLibs/prometheus-2.2.1.linux-amd64/myprometheus.sh
# By default Prometheus will listen in port 9090
# Prometheus pulled the data from services.We need to provide url of services.
# Configuration is there at /home/das/Documents/myLibs/prometheus-2.2.1.linux-amd64/prometheus.yml
------------------------prometheus.yml-------------------------
# my global config
#This scrape_interval value tells Prometheus to collect metrics from its exporters every 15 seconds, 
#which is long enough for most exporters.


global:
  scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).

# Alertmanager configuration
alerting:
  alertmanagers:
  - static_configs:
    - targets:
      # - alertmanager:9093

# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
rule_files:
  # - "first_rules.yml"
  # - "second_rules.yml"

# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
#  - job_name: 'prometheus'

    # metrics_path defaults to '/metrics'
    # scheme defaults to 'http'.

#    static_configs:
#      - targets: ['localhost:9090']
  - job_name: 'brand-source-service'
    metrics_path: '/svcbrands/actuator/prometheus'
    scrape_interval: 5s
    static_configs:
      - targets: ['localhost:8070']
#  - job_name: 'svcone-kub'
#    metrics_path: '/svcone/prometheus'
#    scrape_interval: 5s
#    static_configs:
#      - targets: ['192.168.99.100:31001']
------------------------------------------------------------------------


ElasticSearch Database and kibana
=======================================================================================
## Create a folder in /home/das/Documents/myCmds/elasticsearch/data
## to save elasticsearch data and change the #path.data in elasticsearch.yaml
				or
#keep it default in elasticsearch.yaml. Data will be saved to 
#/home/das/Documents/myLibs/elasticsearch-6.2.2/data

##START elasticsearch & kibana in local(bare metal) , I prefer this for better maintenance

$ /home/das/elk.sh
	#!/bin/bash
	export JAVA_HOME=/home/das/Documents/myLibs/jdk1.8.0_151;
	export PATH=${JAVA_HOME}/bin:${PATH};
	/home/das/Documents/myLibs/elasticsearch-6.2.2/bin/elasticsearch &
	/home/das/Documents/myLibs/kibana-6.2.2-linux-x86_64/bin/kibana &
	
# Check if elasticsearch is UP
http://localhost:9200/
#Kibana
#http://localhost:5601/app/kibana#/home?_g=()


# start kibana in k8s - SOME ISSUE CHECK
kubectl apply -f /home/das/git/k8s-microservice-env-initial-setup/k8s-microservice-env-initial-setup/kibana/kibana.yaml
http://192.168.99.100:32601/
#Get POD logs
k logs <pod name>

	
Start Minikube or GKE
=======================================================================================	
minikube --memory 4000 --cpus 1 start
#http://192.168.99.100:30000/#!/overview?namespace=default

## For GKE
35.185.192.24 
https://console.cloud.google.com/kubernetes/list?project=learn-kubernetes-ddas55&pli=1
gcloud container clusters create dd --num-nodes 2 --machine-type g1-small
gcloud container clusters get-credentials dd

gcloud compute firewall-rules create svc-simpleboot-rule --allow=tcp:12017
k get po -o wide
kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="ExternalIP")].address

Start nginx-ingress in minikube	
=======================================================================================	
#Check the commands from url
#https://kubernetes.github.io/ingress-nginx/deploy/#generic-deployment
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml
kubectl get pods -n ingress-nginx
#we would add a new hostname in the host file of Kubernetes master. 
#If you work on the production environment, a general use case is that the hostname should be added as a record in the DNS server
sudo sh -c "echo `minikube ip` dd.k8s.io >> /etc/hosts"

Start Kubernetes Metrics Server 
=======================================================================================	
Metrics Server is a cluster-wide aggregator of resource usage data, which it collects 
from the Summary API of the kubelets of each node. It registers itself in the main API 
server through Kubernetes Aggregator and thus is discoverable through the same API endpoint 
as the rest of Kubernetes under /apis/metrics.k8s.io/.

#In order to deploy metrics-server in the cluster
kubectl -f create https://github.com/kubernetes/kops/blob/master/addons/metrics-server/v1.8.x.yaml
kubectl get po --namespace kube-system

kubectl get --raw "/apis/metrics.k8s.io/v1beta1/nodes" | jq



CREATE configmap in Minikube
=======================================================================================	
kubectl create configmap cm-kub-brandssource-service \
--from-file=/home/das/git/kub-brandsSource-service/kub-brandsSource-service/src/main/resources/application.properties

kubectl get configmap cm-kub-brandssource-service  -o yaml

# Replace created configMap
kubectl create configmap cm-kub-brandssource-service \
--from-file=/home/das/git/kub-brandsSource-service/kub-brandsSource-service/src/main/resources/application.properties -o yaml --dry-run | kubectl replace -f -

CREATE Secret in Minikube
=======================================================================================	
kubectl create secret generic scrt-mongodb-access \
--from-file=/home/das/git/k8s-microservice-env-initial-setup/k8s-microservice-env-initial-setup/secret/mongodb-secret.txt
kubectl get secret scrt-mongodb-access -o yaml
echo "c3ByaW5nLmRhdGEubW9uZ29kYi5ob3N0PTU0LjcwLjIxMC4yMTIKc3ByaW5nLmRhdGEubW9uZ29kYi5wb3J0PTEyMDE3CnNwcmluZy5kYXRhLm1vbmdvZGIudXNlcm5hbWU9ZGRhcwpzcHJpbmcuZGF0YS5tb25nb2RiLnBhc3N3b3JkPUd1bnRpQDE2T2N0MjAwNQpzcHJpbmcuZGF0YS5tb25nb2RiLmRhdGFiYXNlPUNsaXN0" |base64 --decode


Jenkins inside Tomcat
=======================================================================================
export JAVA_HOME=/home/das/Documents/myLibs/jdk1.8.0_151;
export PATH=${JAVA_HOME}/bin:${PATH};
/home/das/Documents/myLibs/apache-tomcat-9.0.8/bin/startup.sh

#/home/das/Documents/myLibs/apache-tomcat-9.0.8/bin/shutdown.sh 
# http://localhost:8888/jenkins

	


Start fluentd , zipkin
=======================================================================================
# start fluentd In jenkins
Run the job - kub-common-infrastructure
# start zipkin In jenkins
Run the job - kub-common-infrastructure
# kubectl get svc zipkin-collector
# http://192.168.99.100:32323/

# start fluentd - Manual from command line
kubectl apply -f /home/das/git/k8s-microservice-env-initial-setup/k8s-microservice-env-initial-setup/fluentd/fluentd-sa.yml
kubectl apply -f /home/das/git/k8s-microservice-env-initial-setup/k8s-microservice-env-initial-setup/fluentd/fluentd-ds.yml
#Note - If elasticsearch running in localhost m/c and fluentd running in minikube then use name:  FLUENT_ELASTICSEARCH_HOST
# value: "10.0.2.2 . If elasticsearch running in bare metal and k8s cluster is NOT minikube try to provide elasticsearch
# IP in field - FLUENT_ELASTICSEARCH_HOST


Start Grafana
=======================================================================================
## start Grafana In jenkins
Run the job - kub-common-infrastructure
kubectl apply -f /home/das/git/k8s-microservice-env-initial-setup/k8s-microservice-env-initial-setup/grafana/dp-grafana.yaml
kubectl apply -f /home/das/git/k8s-microservice-env-initial-setup/k8s-microservice-env-initial-setup/grafana/svc-grafana.yaml
#http://192.168.99.100:31111
#userid/pwd - admin/admin

## Set Database point to prometheus
Setting --> Data Sources
Name-Prometheus
Type-Prometheus
#HTTP
#If Grafana is running inside minikube provide the url ( http://10.0.2.2:9090) else
#directlot point to host where Prometheus is running
URL-http://10.0.2.2:9090
Access-Default






#Create
cd /home/das/Documents/myCmds/myKubernetes/boot-fluentd-elasticsearch-kibana
k apply -f /home/das/Documents/myCmds/eclipseWorkspace/spring-cloud/kub-cm-mongo-flk-producer/src/main/resources/myrunCmds/elasticsearch/es-config.yml
k apply -f /home/das/Documents/myCmds/eclipseWorkspace/spring-cloud/kub-cm-mongo-flk-producer/src/main/resources/myrunCmds/elasticsearch/es-logging.yml
#Note - If Kibana and elasticsearch running in localhost m/c and running the k8s in minikube then use name:  FLUENT_ELASTICSEARCH_HOST
# value: "10.0.2.2"
  
k apply -f /home/das/Documents/myCmds/eclipseWorkspace/spring-cloud/kub-cm-mongo-flk-producer/src/main/resources/myrunCmds/elasticsearch/kibana-logging.yml
k apply -f /home/das/Documents/myCmds/eclipseWorkspace/spring-cloud/kub-cm-mongo-flk-producer/src/main/resources/myrunCmds/kibana/kibana.yaml

#Stop ELK in local
ps -af | grep kibana*
ps -af | grep elasticsearch
ps -af | grep logstash